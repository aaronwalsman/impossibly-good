Need:
X How to make expert?
    X Rough idea: for each environment, make a path of waypoints that solves it.
        If you are close enough to the path, follow the path.
        If you are not close enough to the path, get to the path.
    X Or alternatively, globally track which waypoint you recently reached, and
        always supervise to first look towards, then move towards the next one.
    X What we ended up doing:
        Each waypoint has:
            A position
            A target waypoint
            "use" and "use_pushed" attributes
                "use" means that use button should be pushed when you reach it
                "use_pushed" means that the use button was already pushed
        Get the closest waypoint
        If you are not pointing at its target waypoint
            Rotate until you are
        If you are pointing at its target waypoint
            If the closest waypoint has "use" and not "use_pushed"
                Push the use button
            Otherwise
                Walk forward

X How to randomize scenarios?
    X Probably build multiple maps and write a wrapper that switches
        between them randomly on reset.

X Need memory (LSTMs) back in there

X Also maybe first start to build what I actually want
    X MONSTER ROOM!

O ELF not working yet
    X On-policy Distill works when the environment starts by looking at the
        correct target.  This implies that the memory and data is ok.
    X ELF fails to work even if we start very close and all it has to do is
        look left.
        X This was the memory issue I think
        O Could be switching time too long?
            O Could also just try automagic shorter switching time at first
                O doesn't seem to help yet?  Running slightly longer?
            O If that helps, we could try early-stopping with switching time
                based on empirical episode lengths (early stopping criteria
                would be like need to travel X units per 12 steps or
                something... oof but turning?)
        O Could be "fancy"?
            O No, but the FOLLOWER doesn't seem to be working
        O The EXPLORER puts 100% weight on walk-forward for the first frame
            could this be a result of fancy training? (totally).  Also if this
            is happening, then we could totally be killing the follower's
            ability to figure out good states (overfitting too early)
            O Maybe increase entropy bonus?
            O Wait, let's just make the exploration phase pick random actions
                for the moment, to see if this is the issue.
                O Seems like this is still not great...
        O Similar to the switching time question, could also be sequences too
            long.  I may just not be getting enough experience of these early
            trajectories?  Still it seems like I should be seeing some multiple
            of performance of the OPD result here though.
        X Also, finish labels are messed up.  They only provide once, even if
            the agent never pushed the button.
        O Explorer learning seems shit
            X Can now learn when starting from staring at the window
        X Turned off the fancy business entirely for now.  The concern is that
            it could be leading to explorer behavior that is too confident,
            which removes exploration.
            X Fancy is back on, but it's the right one now (constant threshold)
        O Let's try making the explorer reaaaaaaly myopic.
            X Ok, so what happens is: the follower does learn to do the right
                thing by around 400k examples, but later at the end... it seems
                bad again.
                X What could be happening?
                    X Ok, I think the memory issue below was partly to blame
                        here, retrying.
            X With myopic explorer, even when starting staring right at the
                window, it seems to just sit there and spam 3.  That's odd,
                right?  Follower works great though.
        X Add 3,3 as an early stopping sequence
        X The explorer is really bad even from starting staring right at the
            window.  Just wiggles back and forth for some reason.
            Even with fancy... why even with fancy?
            X I bet the memory is WRONG!  The follower's value function is not
                doing the right thing BECAUSE it's feeding the EXPLORER's
                memory into the follower.  YEP DEFINITELY THIS.
                X Solve this by making a model that shares a backbone and just
                    has different heads I think.
                    X Ok, now when pointing right at the window, the explorer
                        finally figures it out.
                X This was also definitely an issue with using the explorer
                    when rolling out the follower as well.
                X May need to add a few more layers to the heads... did it, not
                    sure if it was necessary, but it's done
        O Remaining issue: even when only two or three turning steps from the
            window, it looks like the follower is not getting much experience
            seeing that window.  The exploration is just pretty bad.
            O Is there some better early-stopping we can do?
            O Should we add some small negative penalty when early-stopping?
            O Can we add a bonus for distance travelled?  This may actually get
                us to the point where we can solve the version starting at the
                true origin as well?
                O Could be distance travelled or could be bounding box of
                    locations travelled?
            X Tried a thing where we put the agent in the correct starting point
                but I think we're not ready for that yet.
        O An issue that comes up (frequently now) is that the explorer can fall
            into bad policies that never look at the window before the follower
            learns anything.  Maybe up the entropy bonus?
        O Oh shoot, another issue is that the switching time/horizon is not
            correct right now
        X Even fixing all this though, the explorer still seems like it's doing
            something wrong at the moment.  It was able to optimize for the
            distance travelled thing a while ago, but now it seems... bad.
            Even for 2/3 steps away from the window, with follower exploration
            just always turning left, and the follower seems to be learning what
            to do, the explorer is doing...
            X Op, nevermind, it figures out the right thing around 350k frames.
                See 16 as an example.
        X Another thing to be aware of is that the follower sometimes doesn't
            do very well starting on it's own.  This may be because it's not
            used to seeing this, it's used to starting from some exploration
            history before it wanders off on it's own.
        X Uh oh, after the first few steps the follower settles into the same
            action distribution for all frames?  The explorer is also doing
            this???
            X Maybe because with the go somewhere reward shaping, it frequently
                ends up stuck up against walls doing the same action over and
                over?  Maybe do another early stopping where if you go forward
                and don't move more than... 10 units?... it early terminates?
            X Try to pin down when/why policy is collapsing
                X Negative reward
                    X Reproed 16 as 19 with this and it seems good
                X Try learning rate
                    X Seems to be more stable after reducing the learning rate
                    X Yeah, trained 5M frames, and I don't see the collapsing
                N Max grad norm
                N PPO clipping
        X Try more memory (after collapsing)
            X Running now
                X Didn't do it.  For some reason the follower is not learning.
                    Maybe still a memory thing?
        O Alright, what's up with the follower?  Even at about 20% of the way
            through training, it seems like it sees the monster/empty room in
            a lot of training trajectories
            O More expert/early stopping issues:
                X Sometimes instructed to press 3 again after opening the door
                    X New expert should take of this...
                X Had to take out the wall collision thing, again because of
                    the door, but it seems like we still want/need it
                O We should maybe try just using the expert directly
                    when training the value estimator, just to make sure that
                    even the expert can solve the problem
            O Maybe also turn off the early-stopping for the follower?
                Kinda feels like cheating though.
            O Also maybe I can turn off the 1,2,1 and 2,1,2 now that we have
                the exploration bonus?  It seems like the follower is losing
                some experience because this is getting in the way... but maybe
                it would be generating a ton of bad experience without it?
    O At this point it seems like the follower is able to get to the doors
        perfectly.  The distribution is super sharp, takes you right there.
        The problem is that it still doesn't seem to know which door to go in
        even after seeing the monster/clear room.  So it COULD be a memory
        thing, ALTHOUGH, we have shown that the model CAN do the right thing
        when it STARTS looking at the moster (or starts very near it).
        O Could feed in the step id... for some reason?
        O It's possible that all the model capacity is going towards the thing
            that's optimizing the exploration bonus?
        O It's possible that the neurons are just getting totally cooked at
            this point?  What would happen if we trained a fresh model to
            do the follower's job?  The problem is, the follower needs to have
            the same backbone in order to allow continuity from the exploration
            policy.
        O Let's drop the value of mask on the images to make sure there's
            nothing bad going on there too huh?
        O Hey also, are LSTMs bad?
            X Meh?  I made an attention thing that also works, but seems to
                take a while to train (see fe_monster_room_recur_attn_eashish)
        O Batch size too big?  Yeah looks like it works out to about 6 updates
            per chunk of collected data with batch size 256.  Cutting to 64 to
            see how that does.  Maybe increase the frames-per-chunk too?
            O Ok so:
                O 512 frames per proc, recurrence 64, batch_size 1024 yields:
                    O 512 * 16 = 8192 frames per epoch
                    O 1024 // 64 = 16 sequences per batch
                    O 8192 // 1024 = 8 backward passes per epoch
                    O Doesn't even work on easy mode
                    O FPS is almost twice as good though (~200 instead of ~100)
                        (because we have a larger parallel batch size)
                O Before was 128 frames per proc, recurrence 64, batch_size 256
                    O 128 * 16 = 2048 frames per epoch
                    O 256 // 64 = 4 sequences per batch
                    O 2048 // 256 = 8 backward passes per epoch
                    O Worked on easy mode
                O Also trying 512 frames per proc, r 64, batch_size 256
                    O 512 * 16 = 8192 frames per epoch
                    O 256 // 64 = 4 sequences per batch
                    O 8192 // 256 = 32 backward passes per epoch
                    O Testing on easy mode
                O Klemen recommends:
                    128 frames per proc, recurrence 64, batch size 1024:
                    O 128 * 16 = 2048 frames per proc
                    O 1024 // 64 = 16 sequences per batch
                    O 2048 // 1024 = 2 backward passes per epoch
                    O Testing on easy mode
                        O NOPE!
                    O Woah, FPS 300?  Ok, seems great.
        O We might have turned the learning rate down too much?  The version
            16 (and I think 19) that worked well in easy mode I think have the
            higher learning rate
            O With original settings and the learning rate turned down, it
                seems like it's only able to get the follower working in
                easy mode after 700k steps, which is FOREVER... maybe bump this
                a little bit.  This is also only 32 recurrence.
        O The path:
            X Easy mode recurrence=32
                X works entirely with lr=0.001 in 300k frames
                    fe_monster_room_recur_offc16
                    fe_monster_room_recur_offc19
                    (bs 256, steps_per_proc 128, recurrence 32)
                X trains follower with lr=0.0001 in 700-800k frames
                    fe_monster_room_recur_lstm_easy6
                    (bs 256, steps_per_proc 128, recurrence 32)
                X trains follower with lr=0.0005 in 350k frames or so
                    fe_monster_room_recur_lstm_easy7
                    (bs 256, steps_per_proc 128, recurrence 32)
                X trains follower with lr=0.0005 in 750k faster frames
                    fe_monster_room_recur_lstm_easy8
                    (bs 1024, steps_per_proc 128, recurrence 32)
            X Easy mode recurrence=64
                X trains follower with lr=0.0005 in 500k-750k frames
                    fe_monster_room_recur_lstm_easy9
                    (bs 512, steps_per_proc 128, recurrence 64)
                X trains follower with lr=0.0005 in 650k-1M frames
                    fe_monster_room_recur_lstm_easy10
                    (bs 1024, steps_per_proc 128, recurrence 64)
            X Hard mode recurrence=64
                O need to double check switching horizons
                O next version we are going to slightly change the shape of
                    the room so we can see the monster as soon as we turn
                    around at the poison wall
                O may be worth checking if hard2 goes with longer training
                O seems so close, but taking a while... 1.2M/1.8M and still
                    unable to distinguish based on prior history
                O I wonder if I should try shrinking and tanh-ifying the
                    decoders?  Better gradients?
                X IT WORKS IT WORKS IT WORKS!!!
                    X fe_monster_room_recur_lstm_hard3
                        X took probably 1:45 hours (2M) (this one is slower)
                    X fe_monster_room_recur_lstm_hard4
                        X Took 1.5 hours (2.5M)
                O Need to try one more thing: does it work with smaller
                    exploration bonus?
